# Data / preprocessing (must match the pretrained model)
kmer_size: 3   
pretrain_num_classes: 6   
max_seq_length: 2048   
# Model architecture
embedding_dim: 64  # must match the pretrained model
num_heads: 4  # must match the pretrained model
num_layers: 2  # must match the pretrained model
hidden_dim: 192  # must match the pretrained model
dropout_rate: 0.2
hidden_dim_for_binary_classifier: 64
# Training hyperparameters
batch_size: 4
learning_rate: 3e-4
num_epochs: 55
use_class_weights: False
weight_decay: 8e-4
max_grad_norm: 1.0
# LR scheduling
start_factor: 0.01
end_factor: 1.0
warmup_epochs: 12 
lr_min: 1e-4
cosine_cycle_epochs: 55
# OOF pooled Bootstrap for final metrics
n_boot: 500
# Device 
device: "cuda:6"  # or "cpu"
# DataLoader workers
num_workers: 0
# Seed  
seed: 42
# Paths for pretrain checkpoint, kmer vocab and finetune fasta
pretrained_ckpt_path: "pretrain_save_20250923_1727/pretrained_best_model_epoch436.pth"
pretrained_kmer_json: "pretrain_save_20250923_1727/pretrained_kmer_to_idx.json"
finetune_combined_fasta: "../data/preprocessed/finetune/wb_splits/wb_finetune_trainval_set.fasta"
reference_fasta: "../data/raw/mRNA_045_WT.fasta"
# Label type
label: "wb"

#### command line to run wb_cv
# python finetune_cv.py --config finetune_configs/wb_cv_config.yaml
